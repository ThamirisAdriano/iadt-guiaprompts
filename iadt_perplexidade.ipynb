{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUGt6CAuiVhduPfUs5YA8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThamirisAdriano/iadt-prompts/blob/master/iadt_perplexidade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação das bibliotecas:\n",
        "\n",
        "torch: É usada para cálculos matemáticos, especialmente com modelos de aprendizado profundo.\n",
        "\n",
        "transformers: Biblioteca que fornece ferramentas para trabalhar com modelos de linguagem pré-treinados, como o GPT-2.\n",
        "\n",
        "O GPT2Tokenizer e o GPT2LMHeadModel são componentes que fazem a tokenização (converter texto em números que o modelo entende) e carregam o modelo GPT-2."
      ],
      "metadata": {
        "id": "IvHVkubEXPC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H95ojt9glYEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Carregar o tokenizador e o modelo pré-treinado:**\n",
        "\n",
        "O tokenizer é responsável por transformar o texto em uma sequência de números.\n",
        "\n",
        "O model é o GPT-2 pré-treinado, usado para processar o texto."
      ],
      "metadata": {
        "id": "_j5dJHU-XcYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "iKeYitAvXDdU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Função calcular_perplexidade:**\n",
        "\n",
        "Recebe um texto (prompt) como entrada.\n",
        "\n",
        "Converte esse texto em tensores (números que o modelo GPT-2 pode processar) usando o tokenizer.\n",
        "\n",
        "O modelo gera uma previsão e calcula a loss (uma medida de quão bem o modelo previu as próximas palavras no texto).\n",
        "\n",
        "**A perplexidade é obtida aplicando a função exp() à loss.\n",
        "Isso transforma a perda em uma medida de incerteza: uma perplexidade menor significa que o modelo está mais confiante sobre suas previsões.**"
      ],
      "metadata": {
        "id": "0YP2Qk7dXxyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calcular_perplexidade(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "    loss = outputs.loss\n",
        "    perplexidade = torch.exp(loss)\n",
        "    return perplexidade.item()\n",
        "\n"
      ],
      "metadata": {
        "id": "8ClMyS30XIqB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"Descreva a estrutura e usos do ácido acético.\"\n",
        "prompt2 = \"Fale sobre o ácido acético.\"\n",
        "\n",
        "perplexidade1 = calcular_perplexidade(prompt1)\n",
        "perplexidade2 = calcular_perplexidade(prompt2)\n"
      ],
      "metadata": {
        "id": "7lrnNib7XJ5N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Perplexidade do Prompt 1: {perplexidade1}\")\n",
        "print(f\"Perplexidade do Prompt 2: {perplexidade2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx3YhJd6XLUj",
        "outputId": "075943f4-8020-451b-e1c2-a704525ddba7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexidade do Prompt 1: 228.68093872070312\n",
            "Perplexidade do Prompt 2: 440.08837890625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexidade mais baixa (Prompt 1): O modelo encontrou uma sequência de palavras que ele reconheceu mais facilmente. Portanto, o modelo está mais \"confiante\" ao processar o primeiro prompt.\n",
        "\n",
        "Perplexidade mais alta (Prompt 2): O segundo prompt foi mais difícil para o modelo prever, o que pode ocorrer porque ele é mais curto ou genérico, e o modelo fica mais incerto sobre qual direção seguir."
      ],
      "metadata": {
        "id": "4thvinhBYfsK"
      }
    }
  ]
}